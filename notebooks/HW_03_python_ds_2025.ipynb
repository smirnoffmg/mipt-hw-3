{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aca0353b",
   "metadata": {
    "id": "aca0353b"
   },
   "source": [
    "# Домашнее задание 3. Парсинг, Git и тестирование на Python\n",
    "\n",
    "**Цели задания:**\n",
    "\n",
    "* Освоить базовые подходы к web-scraping с библиотеками `requests` и `BeautisulSoup`: навигация по страницам, извлечение HTML-элементов, парсинг.\n",
    "* Научиться автоматизировать задачи с использованием библиотеки `schedule`.\n",
    "* Попрактиковаться в использовании Git и оформлении проектов на GitHub.\n",
    "* Написать и запустить простые юнит-тесты с использованием `pytest`.\n",
    "\n",
    "\n",
    "В этом домашнем задании вы разработаете систему для автоматического сбора данных о книгах с сайта [Books to Scrape](http://books.toscrape.com). Нужно реализовать функции для парсинга всех страниц сайта, извлечения информации о книгах, автоматического ежедневного запуска задачи и сохранения результата.\n",
    "\n",
    "Важной частью задания станет оформление проекта: вы создадите репозиторий на GitHub, оформите `README.md`, добавите артефакты (код, данные, отчеты) и напишете базовые тесты на `pytest`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "K3JMV0qwmA_q",
   "metadata": {
    "id": "K3JMV0qwmA_q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -q schedule pytest # установка библиотек, если ещё не"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "873d4904",
   "metadata": {
    "id": "873d4904"
   },
   "outputs": [],
   "source": [
    "# Библиотеки, которые могут вам понадобиться\n",
    "# При необходимости расширяйте список\n",
    "import time\n",
    "import requests\n",
    "import schedule\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unTvsWaegHdj",
   "metadata": {
    "id": "unTvsWaegHdj"
   },
   "source": [
    "## Задание 1. Сбор данных об одной книге (20 баллов)\n",
    "\n",
    "В этом задании мы начнем подготовку скрипта для парсинга информации о книгах со страниц каталога сайта [Books to Scrape](https://books.toscrape.com/).\n",
    "\n",
    "Для начала реализуйте функцию `get_book_data`, которая будет получать данные о книге с одной страницы (например, с [этой](http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html)). Соберите всю информацию, включая название, цену, рейтинг, количество в наличии, описание и дополнительные характеристики из таблицы Product Information. Результат достаточно вернуть в виде словаря.\n",
    "\n",
    "**Не забывайте про соблюдение PEP-8** — помимо качественно написанного кода важно также документировать функции по стандарту:\n",
    "* кратко описать, что она делает и для чего нужна;\n",
    "* какие входные аргументы принимает, какого они типа и что означают по смыслу;\n",
    "* аналогично описать возвращаемые значения.\n",
    "\n",
    "*P. S. Состав, количество аргументов функции и тип возвращаемого значения можете менять как вам удобно. То, что написано ниже в шаблоне — лишь пример.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "UfD2vAjHkEoS",
   "metadata": {
    "id": "UfD2vAjHkEoS"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BookData:\n",
    "    \"\"\"\n",
    "    Data class to hold book information scraped from Books to Scrape website.\n",
    "    \n",
    "    Attributes:\n",
    "        title: Book title\n",
    "        price: Book price as string (e.g., \"£51.77\")\n",
    "        rating: Star rating (e.g., \"Three\", \"Four\", \"Five\")\n",
    "        availability: Stock availability information\n",
    "        description: Book description text\n",
    "        product_info: Dictionary containing product information table data\n",
    "    \"\"\"\n",
    "    title: str\n",
    "    price: str\n",
    "    rating: str\n",
    "    availability: str\n",
    "    description: str\n",
    "    product_info: Dict[str, str]\n",
    "\n",
    "\n",
    "def _extract_text(soup: BeautifulSoup, tag: str, default: str, **kwargs) -> str:\n",
    "    \"\"\"Universal text extraction from HTML element.\"\"\"\n",
    "    element = soup.find(tag, **kwargs)\n",
    "    return element.get_text(strip=True) if element else default\n",
    "\n",
    "\n",
    "def _extract_attribute(soup: BeautifulSoup, tag: str, attr_name: str, default: str, **kwargs) -> str:\n",
    "    \"\"\"Universal attribute extraction from HTML element.\"\"\"\n",
    "    element = soup.find(tag, **kwargs)\n",
    "    if element and attr_name in element.attrs:\n",
    "        attr_value = element.attrs[attr_name]\n",
    "        if isinstance(attr_value, list) and len(attr_value) > 1:\n",
    "            return attr_value[1]  # For rating: get second class name\n",
    "        return str(attr_value)\n",
    "    return default\n",
    "\n",
    "\n",
    "def _extract_text_from_next_sibling(soup: BeautifulSoup, parent_tag: str, \n",
    "                                   sibling_tag: str, default: str, **kwargs) -> str:\n",
    "    \"\"\"Extract text from next sibling element.\"\"\"\n",
    "    parent = soup.find(parent_tag, **kwargs)\n",
    "    if parent:\n",
    "        sibling = parent.find_next_sibling(sibling_tag)\n",
    "        if sibling:\n",
    "            return sibling.get_text(strip=True)\n",
    "    return default\n",
    "\n",
    "\n",
    "def _extract_table_data(soup: BeautifulSoup, tag: str, **kwargs) -> Dict[str, str]:\n",
    "    \"\"\"Extract key-value pairs from HTML table.\"\"\"\n",
    "    data = {}\n",
    "    table = soup.find(tag, **kwargs)\n",
    "    if table:\n",
    "        rows = table.find_all(\"tr\")\n",
    "        for row in rows:\n",
    "            th = row.find(\"th\")\n",
    "            td = row.find(\"td\")\n",
    "            if th and td:\n",
    "                key = th.get_text(strip=True)\n",
    "                value = td.get_text(strip=True)\n",
    "                data[key] = value\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_book_data(book_url: str) -> Optional[BookData]:\n",
    "    \"\"\"\n",
    "    Scrape book information from a single book page on Books to Scrape website.\n",
    "    \n",
    "    Args:\n",
    "        book_url: URL of the book page to scrape\n",
    "        \n",
    "    Returns:\n",
    "        BookData object containing scraped book information, or None if scraping fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(book_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        return BookData(\n",
    "            title=_extract_text(soup, \"h1\", \"Unknown Title\"),\n",
    "            price=_extract_text(soup, \"p\", \"Price not available\", class_=\"price_color\"),\n",
    "            rating=_extract_attribute(soup, \"p\", \"class\", \"Not rated\", class_=\"star-rating\"),\n",
    "            availability=_extract_text(soup, \"p\", \"Availability unknown\", class_=\"instock availability\"),\n",
    "            description=_extract_text_from_next_sibling(soup, \"div\", \"p\", \"No description available\", id=\"product_description\"),\n",
    "            product_info=_extract_table_data(soup, \"table\", class_=\"table table-striped\")\n",
    "        )\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Network error occurred: {e}\")\n",
    "        return None\n",
    "    except AttributeError as e:\n",
    "        print(f\"Parsing error occurred: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "moRSO9Itp1LT",
   "metadata": {
    "id": "moRSO9Itp1LT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Book: A Light in the Attic\n",
      "Price: £51.77\n",
      "Rating: Three\n",
      "Availability: In stock (22 available)\n",
      "\n",
      "Description:\n",
      "It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverste...\n",
      "\n",
      "Product Information:\n",
      "  UPC: a897fe39b1053632\n",
      "  Product Type: Books\n",
      "  Price (excl. tax): £51.77\n",
      "  Price (incl. tax): £51.77\n",
      "  Tax: £0.00\n",
      "  Availability: In stock (22 available)\n",
      "  Number of reviews: 0\n"
     ]
    }
   ],
   "source": [
    "# Test the function with the provided URL\n",
    "book_url = 'http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html'\n",
    "book_data = get_book_data(book_url)\n",
    "\n",
    "if book_data:\n",
    "    print(f\"\\nBook: {book_data.title}\")\n",
    "    print(f\"Price: {book_data.price}\")\n",
    "    print(f\"Rating: {book_data.rating}\")\n",
    "    print(f\"Availability: {book_data.availability}\")\n",
    "    print(\"\\nDescription:\")\n",
    "    print(f\"{book_data.description[:200]}...\" if len(book_data.description) > 200 else book_data.description)\n",
    "    print(\"\\nProduct Information:\")\n",
    "    for key, value in book_data.product_info.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"Failed to scrape book data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u601Q4evosq6",
   "metadata": {
    "id": "u601Q4evosq6"
   },
   "source": [
    "## Задание 2. Сбор данных обо всех книгах (20 баллов)\n",
    "\n",
    "Создайте функцию `scrape_books`, которая будет проходиться по всем страницам из каталога (вида `http://books.toscrape.com/catalogue/page-{N}.html`) и осуществлять парсинг всех страниц в цикле, используя ранее написанную `get_book_data`.\n",
    "\n",
    "Добавьте аргумент-флаг, который будет отвечать за сохранение результата в файл: если он будет равен `True`, то информация сохранится в ту же папку в файл `books_data.txt`; иначе шаг сохранения будет пропущен.\n",
    "\n",
    "**Также не забывайте про соблюдение PEP-8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "kk78l6oDkdxl",
   "metadata": {
    "id": "kk78l6oDkdxl"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dataclasses import asdict\n",
    "\n",
    "\n",
    "def _collect_book_urls() -> List[str]:\n",
    "    \"\"\"\n",
    "    Collect all book URLs by iterating through catalog pages.\n",
    "    \n",
    "    Returns:\n",
    "        List of absolute book URLs\n",
    "    \"\"\"\n",
    "    book_urls = []\n",
    "    page = 1\n",
    "    base_url = \"http://books.toscrape.com/catalogue\"\n",
    "    \n",
    "    while True:\n",
    "        # Construct catalog page URL - start with page-1.html\n",
    "        page_url = f\"{base_url}/page-{page}.html\"\n",
    "        \n",
    "        try:\n",
    "            # Fetch and parse catalog page\n",
    "            response = requests.get(page_url, timeout=10)\n",
    "            if response.status_code != 200:\n",
    "                break\n",
    "                \n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            \n",
    "            # Find all book links on this catalog page\n",
    "            articles = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "            if not articles:\n",
    "                break\n",
    "                \n",
    "            for article in articles:\n",
    "                # Extract book URL from article\n",
    "                link_element = article.find(\"h3\").find(\"a\")\n",
    "                if link_element:\n",
    "                    link = link_element[\"href\"]\n",
    "                    # Convert relative to absolute URL\n",
    "                    absolute_url = f\"http://books.toscrape.com/catalogue/{link}\"\n",
    "                    book_urls.append(absolute_url)\n",
    "            \n",
    "            print(f\"Catalog page {page}: found {len(articles)} books\")\n",
    "            page += 1\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching catalog page {page}: {e}\")\n",
    "            break\n",
    "    \n",
    "    return book_urls\n",
    "\n",
    "\n",
    "def scrape_books(is_save: bool = False, max_workers: int = 10) -> List[BookData]:\n",
    "    \"\"\"\n",
    "    Scrape all books from Books to Scrape catalog using concurrent execution.\n",
    "    \n",
    "    Args:\n",
    "        is_save: If True, save results to books_data.txt\n",
    "        max_workers: Number of concurrent workers for parallel scraping\n",
    "        \n",
    "    Returns:\n",
    "        List of BookData objects for all scraped books\n",
    "    \"\"\"\n",
    "    # Phase 1: Collect all book URLs from catalog pages\n",
    "    print(\"Collecting book URLs from catalog pages...\")\n",
    "    book_urls = _collect_book_urls()\n",
    "    print(f\"Found {len(book_urls)} books to scrape\")\n",
    "    \n",
    "    if not book_urls:\n",
    "        print(\"No books found to scrape\")\n",
    "        return []\n",
    "    \n",
    "    # Phase 2: Scrape books concurrently\n",
    "    print(f\"Scraping books with {max_workers} workers...\")\n",
    "    books = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_url = {executor.submit(get_book_data, url): url for url in book_urls}\n",
    "        \n",
    "        completed = 0\n",
    "        for future in as_completed(future_to_url):\n",
    "            book_data = future.result()\n",
    "            if book_data:\n",
    "                books.append(book_data)\n",
    "            completed += 1\n",
    "            if completed % 100 == 0:\n",
    "                print(f\"Progress: {completed}/{len(book_urls)} books scraped\")\n",
    "    \n",
    "    print(f\"Successfully scraped {len(books)} books\")\n",
    "    \n",
    "    # Save to file if requested\n",
    "    if is_save:\n",
    "        output_path = Path(\"books_data.txt\")\n",
    "        \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump([asdict(book) for book in books], f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Saved to {output_path}\")\n",
    "    \n",
    "    return books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "Bt7mrXcbkj5Q",
   "metadata": {
    "id": "Bt7mrXcbkj5Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing scrape_books function...\n",
      "Collecting book URLs from catalog pages...\n",
      "Catalog page 1: found 20 books\n",
      "Catalog page 2: found 20 books\n",
      "Catalog page 3: found 20 books\n",
      "Catalog page 4: found 20 books\n",
      "Catalog page 5: found 20 books\n",
      "Catalog page 6: found 20 books\n",
      "Catalog page 7: found 20 books\n",
      "Catalog page 8: found 20 books\n",
      "Catalog page 9: found 20 books\n",
      "Catalog page 10: found 20 books\n",
      "Catalog page 11: found 20 books\n",
      "Catalog page 12: found 20 books\n",
      "Catalog page 13: found 20 books\n",
      "Catalog page 14: found 20 books\n",
      "Catalog page 15: found 20 books\n",
      "Catalog page 16: found 20 books\n",
      "Catalog page 17: found 20 books\n",
      "Catalog page 18: found 20 books\n",
      "Catalog page 19: found 20 books\n",
      "Catalog page 20: found 20 books\n",
      "Catalog page 21: found 20 books\n",
      "Catalog page 22: found 20 books\n",
      "Catalog page 23: found 20 books\n",
      "Catalog page 24: found 20 books\n",
      "Catalog page 25: found 20 books\n",
      "Catalog page 26: found 20 books\n",
      "Catalog page 27: found 20 books\n",
      "Catalog page 28: found 20 books\n",
      "Catalog page 29: found 20 books\n",
      "Catalog page 30: found 20 books\n",
      "Catalog page 31: found 20 books\n",
      "Catalog page 32: found 20 books\n",
      "Catalog page 33: found 20 books\n",
      "Catalog page 34: found 20 books\n",
      "Catalog page 35: found 20 books\n",
      "Catalog page 36: found 20 books\n",
      "Catalog page 37: found 20 books\n",
      "Catalog page 38: found 20 books\n",
      "Catalog page 39: found 20 books\n",
      "Catalog page 40: found 20 books\n",
      "Catalog page 41: found 20 books\n",
      "Catalog page 42: found 20 books\n",
      "Catalog page 43: found 20 books\n",
      "Catalog page 44: found 20 books\n",
      "Catalog page 45: found 20 books\n",
      "Catalog page 46: found 20 books\n",
      "Catalog page 47: found 20 books\n",
      "Catalog page 48: found 20 books\n",
      "Catalog page 49: found 20 books\n",
      "Catalog page 50: found 20 books\n",
      "Found 1000 books to scrape\n",
      "Scraping books with 5 workers...\n",
      "Progress: 100/1000 books scraped\n",
      "Progress: 200/1000 books scraped\n",
      "Progress: 300/1000 books scraped\n",
      "Progress: 400/1000 books scraped\n",
      "Progress: 500/1000 books scraped\n",
      "Progress: 600/1000 books scraped\n",
      "Progress: 700/1000 books scraped\n",
      "Progress: 800/1000 books scraped\n",
      "Progress: 900/1000 books scraped\n",
      "Progress: 1000/1000 books scraped\n",
      "Successfully scraped 1000 books\n",
      "Saved to books_data.txt\n",
      "\n",
      "Result type: <class 'list'>\n",
      "Number of books scraped: 1000\n",
      "\n",
      "First book example:\n",
      "  Title: A Light in the Attic\n",
      "  Price: £51.77\n",
      "  Rating: Three\n"
     ]
    }
   ],
   "source": [
    "# Test the scrape_books function with a small number of workers for testing\n",
    "print(\"Testing scrape_books function...\")\n",
    "res = scrape_books(is_save=True, max_workers=5)\n",
    "print(f\"\\nResult type: {type(res)}\")\n",
    "print(f\"Number of books scraped: {len(res)}\")\n",
    "\n",
    "if res:\n",
    "    print(\"\\nFirst book example:\")\n",
    "    print(f\"  Title: {res[0].title}\")\n",
    "    print(f\"  Price: {res[0].price}\")\n",
    "    print(f\"  Rating: {res[0].rating}\")\n",
    "else:\n",
    "    print(\"No books were scraped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z5fd728nl8a8",
   "metadata": {
    "id": "z5fd728nl8a8"
   },
   "source": [
    "## Задание 3. Настройка регулярной выгрузки (10 баллов)\n",
    "\n",
    "Настройте автоматический запуск функции сбора данных каждый день в 19:00.\n",
    "Для автоматизации используйте библиотеку `schedule`. Функция должна запускаться в указанное время и сохранять обновленные данные в текстовый файл.\n",
    "\n",
    "\n",
    "\n",
    "Бесконечный цикл должен обеспечивать постоянное ожидание времени для запуска задачи и выполнять ее по расписанию. Однако чтобы не перегружать систему, стоит подумать о том, чтобы выполнять проверку нужного времени не постоянно, а раз в какой-то промежуток. В этом вам может помочь `time.sleep(...)`.\n",
    "\n",
    "Проверьте работоспособность кода локально на любом времени чч:мм.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "SajRRCj4n8BZ",
   "metadata": {
    "id": "SajRRCj4n8BZ"
   },
   "outputs": [],
   "source": [
    "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "\n",
    "def run_scheduler():\n",
    "    \"\"\"\n",
    "    Set up and run the automated daily data collection scheduler.\n",
    "    \n",
    "    The scheduler runs the book scraping function every day at 19:00 (7 PM)\n",
    "    and saves the results to books_data.txt. The function runs in an infinite\n",
    "    loop with 60-second intervals to check for scheduled tasks.\n",
    "    \n",
    "    The scheduler can be stopped with Ctrl+C (KeyboardInterrupt).\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    def scheduled_scraping():\n",
    "        \"\"\"Wrapper function for scheduled book scraping.\"\"\"\n",
    "        print(f\"\\n[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Starting scheduled book scraping...\")\n",
    "        \n",
    "        try:\n",
    "            # Run the scraping function with file saving enabled\n",
    "            books = scrape_books(is_save=True, max_workers=5)\n",
    "            print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Successfully scraped {len(books)} books\")\n",
    "            print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Data saved to books_data.txt\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Error during scheduled scraping: {e}\")\n",
    "    \n",
    "    # Schedule the scraping function to run daily at 19:00\n",
    "    schedule.every().day.at(\"19:00\").do(scheduled_scraping)\n",
    "    \n",
    "    print(\"Scheduler started! Book scraping is scheduled for 19:00 daily.\")\n",
    "    print(\"Press Ctrl+C to stop the scheduler.\")\n",
    "    \n",
    "    try:\n",
    "        # Infinite loop to check for scheduled tasks\n",
    "        while True:\n",
    "            schedule.run_pending()\n",
    "            time.sleep(60)  # Check every minute to avoid overloading the system\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nScheduler stopped by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Scheduler error: {e}\")\n",
    "\n",
    "\n",
    "def test_scheduler_with_custom_time(test_time: str = \"14:30\"):\n",
    "    \"\"\"\n",
    "    Test the scheduler functionality with a custom time for immediate verification.\n",
    "    \n",
    "    Args:\n",
    "        test_time: Time in HH:MM format to schedule the test run (default: \"14:30\")\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    def test_scraping():\n",
    "        \"\"\"Test function for scheduler verification.\"\"\"\n",
    "        print(f\"\\n[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] TEST: Starting scheduled scraping at {test_time}...\")\n",
    "        \n",
    "        try:\n",
    "            # Run a small test with just a few books\n",
    "            print(\"Running test with limited scope...\")\n",
    "            books = scrape_books(is_save=False, max_workers=2)\n",
    "            print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] TEST: Successfully scraped {len(books)} books\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] TEST: Error during test scraping: {e}\")\n",
    "    \n",
    "    # Clear any existing schedules\n",
    "    schedule.clear()\n",
    "    \n",
    "    # Schedule the test function at the specified time\n",
    "    schedule.every().day.at(test_time).do(test_scraping)\n",
    "    \n",
    "    current_time = datetime.now().strftime('%H:%M')\n",
    "    print(f\"Test scheduler started! Test scraping is scheduled for {test_time} (current time: {current_time})\")\n",
    "    print(\"Press Ctrl+C to stop the test scheduler.\")\n",
    "    \n",
    "    try:\n",
    "        # Run for a limited time to test the functionality\n",
    "        start_time = time.time()\n",
    "        while time.time() - start_time < 300:  # Run for 5 minutes max\n",
    "            schedule.run_pending()\n",
    "            time.sleep(10)  # Check every 10 seconds for testing\n",
    "            \n",
    "        print(\"Test scheduler completed (5-minute timeout)\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTest scheduler stopped by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Test scheduler error: {e}\")\n",
    "\n",
    "\n",
    "# КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16e94621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing scheduler with custom time (next minute)...\n",
      "Scheduling test for 21:32\n",
      "Test scheduler started! Test scraping is scheduled for 21:32 (current time: 21:31)\n",
      "Press Ctrl+C to stop the test scheduler.\n",
      "\n",
      "[2025-10-28 21:32:04] TEST: Starting scheduled scraping at 21:32...\n",
      "Running test with limited scope...\n",
      "Collecting book URLs from catalog pages...\n",
      "Catalog page 1: found 20 books\n",
      "Catalog page 2: found 20 books\n",
      "Catalog page 3: found 20 books\n",
      "Catalog page 4: found 20 books\n",
      "Catalog page 5: found 20 books\n",
      "Catalog page 6: found 20 books\n",
      "Catalog page 7: found 20 books\n",
      "Catalog page 8: found 20 books\n",
      "Catalog page 9: found 20 books\n",
      "Catalog page 10: found 20 books\n",
      "Catalog page 11: found 20 books\n",
      "Catalog page 12: found 20 books\n",
      "Catalog page 13: found 20 books\n",
      "Catalog page 14: found 20 books\n",
      "Catalog page 15: found 20 books\n",
      "Catalog page 16: found 20 books\n",
      "Catalog page 17: found 20 books\n",
      "Catalog page 18: found 20 books\n",
      "Catalog page 19: found 20 books\n",
      "Catalog page 20: found 20 books\n",
      "Catalog page 21: found 20 books\n",
      "Catalog page 22: found 20 books\n",
      "Catalog page 23: found 20 books\n",
      "Catalog page 24: found 20 books\n",
      "Catalog page 25: found 20 books\n",
      "Catalog page 26: found 20 books\n",
      "Catalog page 27: found 20 books\n",
      "Catalog page 28: found 20 books\n",
      "Catalog page 29: found 20 books\n",
      "Catalog page 30: found 20 books\n",
      "Catalog page 31: found 20 books\n",
      "Catalog page 32: found 20 books\n",
      "Catalog page 33: found 20 books\n",
      "Catalog page 34: found 20 books\n",
      "Catalog page 35: found 20 books\n",
      "Catalog page 36: found 20 books\n",
      "Catalog page 37: found 20 books\n",
      "Catalog page 38: found 20 books\n",
      "Catalog page 39: found 20 books\n",
      "Catalog page 40: found 20 books\n",
      "Catalog page 41: found 20 books\n",
      "Catalog page 42: found 20 books\n",
      "Catalog page 43: found 20 books\n",
      "Catalog page 44: found 20 books\n",
      "Catalog page 45: found 20 books\n",
      "Catalog page 46: found 20 books\n",
      "Catalog page 47: found 20 books\n",
      "Catalog page 48: found 20 books\n",
      "Catalog page 49: found 20 books\n",
      "Catalog page 50: found 20 books\n",
      "Found 1000 books to scrape\n",
      "Scraping books with 2 workers...\n",
      "Progress: 100/1000 books scraped\n",
      "Progress: 200/1000 books scraped\n",
      "Progress: 300/1000 books scraped\n",
      "Progress: 400/1000 books scraped\n",
      "Progress: 500/1000 books scraped\n",
      "Progress: 600/1000 books scraped\n",
      "Progress: 700/1000 books scraped\n",
      "Progress: 800/1000 books scraped\n",
      "Progress: 900/1000 books scraped\n",
      "Progress: 1000/1000 books scraped\n",
      "Successfully scraped 1000 books\n",
      "[2025-10-28 21:35:55] TEST: Successfully scraped 1000 books\n",
      "Test scheduler completed (5-minute timeout)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "print(\"Testing scheduler with custom time (next minute)...\")\n",
    "\n",
    "next_minute = (datetime.now() + timedelta(minutes=1)).strftime('%H:%M')\n",
    "test_scheduler_with_custom_time(next_minute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XFiPtEyaoLxq",
   "metadata": {
    "id": "XFiPtEyaoLxq"
   },
   "source": [
    "## Задание 4. Написание автотестов (15 баллов)\n",
    "\n",
    "Создайте минимум три автотеста для ключевых функций парсинга — например, `get_book_data` и `scrape_books`. Идеи проверок (можете использовать свои):\n",
    "\n",
    "* данные о книге возвращаются в виде словаря с нужными ключами;\n",
    "* список ссылок или количество собранных книг соответствует ожиданиям;\n",
    "* значения отдельных полей (например, `title`) корректны.\n",
    "\n",
    "Оформите тесты в отдельном скрипте `tests/test_scraper.py`, используйте библиотеку `pytest`. Убедитесь, что тесты проходят успешно при запуске из терминала командой `pytest`.\n",
    "\n",
    "Также выведите результат их выполнения в ячейке ниже.\n",
    "\n",
    "**Не забывайте про соблюдение PEP-8**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "lBFAw4b3z8QY",
   "metadata": {
    "id": "lBFAw4b3z8QY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0 -- /Users/m/edu/mipt/mipt-hw-3/.venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/m/edu/mipt/mipt-hw-3\n",
      "plugins: timeout-2.4.0\n",
      "collected 8 items                                                              \u001b[0m\n",
      "\n",
      "../tests/test_scraper.py::TestGetBookData::test_get_book_data_returns_dict_with_required_keys \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "../tests/test_scraper.py::TestGetBookData::test_book_data_fields_are_correct \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "../tests/test_scraper.py::TestGetBookData::test_get_book_data_handles_network_error \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "../tests/test_scraper.py::TestScrapeBooks::test_scrape_books_returns_list \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "../tests/test_scraper.py::TestScrapeBooks::test_scrape_books_handles_empty_catalog \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "../tests/test_scraper.py::TestScrapeBooks::test_scrape_books_handles_failed_scraping \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "../tests/test_scraper.py::TestCollectBookUrls::test_collect_book_urls_returns_list \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "../tests/test_scraper.py::TestCollectBookUrls::test_collect_book_urls_handles_no_books \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m8 passed\u001b[0m\u001b[32m in 0.16s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Ячейка для демонстрации работоспособности\n",
    "# Сам код напишите в отдельном скрипте\n",
    "! pytest ../tests/test_scraper.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cRSQlHfRtOdN",
   "metadata": {
    "id": "cRSQlHfRtOdN"
   },
   "source": [
    "## Задание 5. Оформление проекта на GitHub и работа с Git (35 баллов)\n",
    "\n",
    "В этом задании нужно воспользоваться системой контроля версий Git и платформой GitHub для хранения и управления своим проектом. **Ссылку на свой репозиторий пришлите в форме для сдачи ответа.**\n",
    "\n",
    "### Пошаговая инструкция и задания\n",
    "\n",
    "**1. Установите Git на свой компьютер.**\n",
    "\n",
    "* Для Windows: [скачайте установщик](https://git-scm.com/downloads) и выполните установку.\n",
    "* Для macOS:\n",
    "\n",
    "  ```\n",
    "  brew install git\n",
    "  ```\n",
    "* Для Linux:\n",
    "\n",
    "  ```\n",
    "  sudo apt update\n",
    "  sudo apt install git\n",
    "  ```\n",
    "\n",
    "**2. Настройте имя пользователя и email.**\n",
    "\n",
    "Это нужно для подписи ваших коммитов, сделайте в терминале через `git config ...`.\n",
    "\n",
    "**3. Создайте аккаунт на GitHub**, если у вас его еще нет:\n",
    "[https://github.com](https://github.com)\n",
    "\n",
    "**4. Создайте новый репозиторий на GitHub:**\n",
    "\n",
    "* Найдите кнопку **New repository**.\n",
    "* Укажите название, краткое описание, выберите тип **Public** (чтобы мы могли проверить ДЗ).\n",
    "* Не ставьте галочку Initialize this repository with a README.\n",
    "\n",
    "**5. Создайте локальную папку с проектом.** Можно в терминале, можно через UI, это не имеет значения.\n",
    "\n",
    "**6. Инициализируйте Git в этой папке.** Здесь уже придется воспользоваться некоторой командой в терминале.\n",
    "\n",
    "**7. Привяжите локальный репозиторий к удаленному на GitHub.**\n",
    "\n",
    "**8. Создайте ветку разработки.** По умолчанию вы будете находиться в ветке `main`, создайте и переключитесь на ветку `hw-books-parser`.\n",
    "\n",
    "**9. Добавьте в проект следующие файлы и папки:**\n",
    "\n",
    "* `scraper.py` — ваш основной скрипт для сбора данных.\n",
    "* `README.md` — файл с кратким описанием проекта:\n",
    "\n",
    "  * цель;\n",
    "  * инструкции по запуску;\n",
    "  * список используемых библиотек.\n",
    "* `requirements.txt` — файл со списком зависимостей, необходимых для проекта (не присылайте все из глобального окружения, создайте изолированную виртуальную среду, добавьте в нее все нужное для проекта и получите список библиотек через `pip freeze`).\n",
    "* `artifacts/` — папка с результатами парсинга (`books_data.txt` — полностью или его часть, если весь не поместится на GitHub).\n",
    "* `notebooks/` — папка с заполненным ноутбуком `HW_03_python_ds_2025.ipynb` и запущенными ячейками с выводами на экран.\n",
    "* `tests/` — папка с тестами на `pytest`, оформите их в формате скрипта(-ов) с расширением `.py`.\n",
    "* `.gitignore` — стандартный файл, который позволит исключить временные файлы при добавлении в отслеживаемые (например, `__pycache__/`, `.DS_Store`, `*.pyc`, `venv/` и др.).\n",
    "\n",
    "\n",
    "**10. Сделайте коммит.**\n",
    "\n",
    "**11. Отправьте свою ветку на GitHub.**\n",
    "\n",
    "**12. Создайте Pull Request:**\n",
    "\n",
    "* Перейдите в репозиторий на GitHub.\n",
    "* Нажмите кнопку **Compare & pull request**.\n",
    "* Укажите, что было добавлено, и нажмите **Create pull request**.\n",
    "\n",
    "**13. Выполните слияние Pull Request:**\n",
    "\n",
    "* Убедитесь, что нет конфликтов.\n",
    "* Нажмите **Merge pull request**, затем **Confirm merge**.\n",
    "\n",
    "**14. Скачайте изменения из основной ветки локально.**\n",
    "\n",
    "\n",
    "\n",
    "### Требования к итоговому репозиторию\n",
    "\n",
    "* Файл `scraper.py` с рабочим кодом парсера.\n",
    "* `README.md` с описанием проекта и инструкцией по запуску.\n",
    "* Папка `artifacts/` с результатом сбора данных (`.txt` файл).\n",
    "* Папка `tests/` с тестами на `pytest`.\n",
    "* Папка `notebooks/` с заполненным ноутбуком `HW_03_python_ds_2025.ipynb`.\n",
    "* Pull Request с комментарием из ветки `hw-books-parser` в ветку `main`.\n",
    "* Примерная структура:\n",
    "\n",
    "  ```\n",
    "  books_scraper/\n",
    "  ├── artifacts/\n",
    "  │   └── books_data.txt\n",
    "  ├── notebooks/\n",
    "  │   └── HW_03_python_ds_2025.ipynb\n",
    "  ├── scraper.py\n",
    "  ├── README.md\n",
    "  ├── tests/\n",
    "  │   └── test_scraper.py\n",
    "  ├── .gitignore\n",
    "  └── requirements.txt\n",
    "  ```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
