{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aca0353b",
   "metadata": {
    "id": "aca0353b"
   },
   "source": [
    "# Домашнее задание 3. Парсинг, Git и тестирование на Python\n",
    "\n",
    "**Цели задания:**\n",
    "\n",
    "* Освоить базовые подходы к web-scraping с библиотеками `requests` и `BeautisulSoup`: навигация по страницам, извлечение HTML-элементов, парсинг.\n",
    "* Научиться автоматизировать задачи с использованием библиотеки `schedule`.\n",
    "* Попрактиковаться в использовании Git и оформлении проектов на GitHub.\n",
    "* Написать и запустить простые юнит-тесты с использованием `pytest`.\n",
    "\n",
    "\n",
    "В этом домашнем задании вы разработаете систему для автоматического сбора данных о книгах с сайта [Books to Scrape](http://books.toscrape.com). Нужно реализовать функции для парсинга всех страниц сайта, извлечения информации о книгах, автоматического ежедневного запуска задачи и сохранения результата.\n",
    "\n",
    "Важной частью задания станет оформление проекта: вы создадите репозиторий на GitHub, оформите `README.md`, добавите артефакты (код, данные, отчеты) и напишете базовые тесты на `pytest`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K3JMV0qwmA_q",
   "metadata": {
    "id": "K3JMV0qwmA_q"
   },
   "outputs": [],
   "source": [
    "! pip install -q schedule pytest # установка библиотек, если ещё не"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873d4904",
   "metadata": {
    "id": "873d4904"
   },
   "outputs": [],
   "source": [
    "# Библиотеки, которые могут вам понадобиться\n",
    "# При необходимости расширяйте список\n",
    "import time\n",
    "import requests\n",
    "import schedule\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unTvsWaegHdj",
   "metadata": {
    "id": "unTvsWaegHdj"
   },
   "source": [
    "## Задание 1. Сбор данных об одной книге (20 баллов)\n",
    "\n",
    "В этом задании мы начнем подготовку скрипта для парсинга информации о книгах со страниц каталога сайта [Books to Scrape](https://books.toscrape.com/).\n",
    "\n",
    "Для начала реализуйте функцию `get_book_data`, которая будет получать данные о книге с одной страницы (например, с [этой](http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html)). Соберите всю информацию, включая название, цену, рейтинг, количество в наличии, описание и дополнительные характеристики из таблицы Product Information. Результат достаточно вернуть в виде словаря.\n",
    "\n",
    "**Не забывайте про соблюдение PEP-8** — помимо качественно написанного кода важно также документировать функции по стандарту:\n",
    "* кратко описать, что она делает и для чего нужна;\n",
    "* какие входные аргументы принимает, какого они типа и что означают по смыслу;\n",
    "* аналогично описать возвращаемые значения.\n",
    "\n",
    "*P. S. Состав, количество аргументов функции и тип возвращаемого значения можете менять как вам удобно. То, что написано ниже в шаблоне — лишь пример.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UfD2vAjHkEoS",
   "metadata": {
    "id": "UfD2vAjHkEoS"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BookData:\n",
    "    \"\"\"\n",
    "    Data class to hold book information scraped from Books to Scrape website.\n",
    "    \n",
    "    Attributes:\n",
    "        title: Book title\n",
    "        price: Book price as string (e.g., \"£51.77\")\n",
    "        rating: Star rating (e.g., \"Three\", \"Four\", \"Five\")\n",
    "        availability: Stock availability information\n",
    "        description: Book description text\n",
    "        product_info: Dictionary containing product information table data\n",
    "    \"\"\"\n",
    "    title: str\n",
    "    price: str\n",
    "    rating: str\n",
    "    availability: str\n",
    "    description: str\n",
    "    product_info: Dict[str, str]\n",
    "\n",
    "\n",
    "def _extract_text(soup: BeautifulSoup, tag: str, default: str, **kwargs) -> str:\n",
    "    \"\"\"Universal text extraction from HTML element.\"\"\"\n",
    "    element = soup.find(tag, **kwargs)\n",
    "    return element.get_text(strip=True) if element else default\n",
    "\n",
    "\n",
    "def _extract_attribute(soup: BeautifulSoup, tag: str, attr_name: str, default: str, **kwargs) -> str:\n",
    "    \"\"\"Universal attribute extraction from HTML element.\"\"\"\n",
    "    element = soup.find(tag, **kwargs)\n",
    "    if element and attr_name in element.attrs:\n",
    "        attr_value = element.attrs[attr_name]\n",
    "        if isinstance(attr_value, list) and len(attr_value) > 1:\n",
    "            return attr_value[1]  # For rating: get second class name\n",
    "        return str(attr_value)\n",
    "    return default\n",
    "\n",
    "\n",
    "def _extract_text_from_next_sibling(soup: BeautifulSoup, parent_tag: str, \n",
    "                                   sibling_tag: str, default: str, **kwargs) -> str:\n",
    "    \"\"\"Extract text from next sibling element.\"\"\"\n",
    "    parent = soup.find(parent_tag, **kwargs)\n",
    "    if parent:\n",
    "        sibling = parent.find_next_sibling(sibling_tag)\n",
    "        if sibling:\n",
    "            return sibling.get_text(strip=True)\n",
    "    return default\n",
    "\n",
    "\n",
    "def _extract_table_data(soup: BeautifulSoup, tag: str, **kwargs) -> Dict[str, str]:\n",
    "    \"\"\"Extract key-value pairs from HTML table.\"\"\"\n",
    "    data = {}\n",
    "    table = soup.find(tag, **kwargs)\n",
    "    if table:\n",
    "        rows = table.find_all(\"tr\")\n",
    "        for row in rows:\n",
    "            th = row.find(\"th\")\n",
    "            td = row.find(\"td\")\n",
    "            if th and td:\n",
    "                key = th.get_text(strip=True)\n",
    "                value = td.get_text(strip=True)\n",
    "                data[key] = value\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_book_data(book_url: str) -> Optional[BookData]:\n",
    "    \"\"\"\n",
    "    Scrape book information from a single book page on Books to Scrape website.\n",
    "    \n",
    "    Args:\n",
    "        book_url: URL of the book page to scrape\n",
    "        \n",
    "    Returns:\n",
    "        BookData object containing scraped book information, or None if scraping fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(book_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        return BookData(\n",
    "            title=_extract_text(soup, \"h1\", \"Unknown Title\"),\n",
    "            price=_extract_text(soup, \"p\", \"Price not available\", class_=\"price_color\"),\n",
    "            rating=_extract_attribute(soup, \"p\", \"class\", \"Not rated\", class_=\"star-rating\"),\n",
    "            availability=_extract_text(soup, \"p\", \"Availability unknown\", class_=\"instock availability\"),\n",
    "            description=_extract_text_from_next_sibling(soup, \"div\", \"p\", \"No description available\", id=\"product_description\"),\n",
    "            product_info=_extract_table_data(soup, \"table\", class_=\"table table-striped\")\n",
    "        )\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Network error occurred: {e}\")\n",
    "        return None\n",
    "    except AttributeError as e:\n",
    "        print(f\"Parsing error occurred: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moRSO9Itp1LT",
   "metadata": {
    "id": "moRSO9Itp1LT"
   },
   "outputs": [],
   "source": [
    "# Test the function with the provided URL\n",
    "book_url = 'http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html'\n",
    "book_data = get_book_data(book_url)\n",
    "\n",
    "if book_data:\n",
    "    print(f\"\\nBook: {book_data.title}\")\n",
    "    print(f\"Price: {book_data.price}\")\n",
    "    print(f\"Rating: {book_data.rating}\")\n",
    "    print(f\"Availability: {book_data.availability}\")\n",
    "    print(\"\\nDescription:\")\n",
    "    print(f\"{book_data.description[:200]}...\" if len(book_data.description) > 200 else book_data.description)\n",
    "    print(\"\\nProduct Information:\")\n",
    "    for key, value in book_data.product_info.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"Failed to scrape book data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u601Q4evosq6",
   "metadata": {
    "id": "u601Q4evosq6"
   },
   "source": [
    "## Задание 2. Сбор данных обо всех книгах (20 баллов)\n",
    "\n",
    "Создайте функцию `scrape_books`, которая будет проходиться по всем страницам из каталога (вида `http://books.toscrape.com/catalogue/page-{N}.html`) и осуществлять парсинг всех страниц в цикле, используя ранее написанную `get_book_data`.\n",
    "\n",
    "Добавьте аргумент-флаг, который будет отвечать за сохранение результата в файл: если он будет равен `True`, то информация сохранится в ту же папку в файл `books_data.txt`; иначе шаг сохранения будет пропущен.\n",
    "\n",
    "**Также не забывайте про соблюдение PEP-8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kk78l6oDkdxl",
   "metadata": {
    "id": "kk78l6oDkdxl"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dataclasses import asdict\n",
    "\n",
    "\n",
    "def _collect_book_urls() -> List[str]:\n",
    "    \"\"\"\n",
    "    Collect all book URLs by iterating through catalog pages.\n",
    "    \n",
    "    Returns:\n",
    "        List of absolute book URLs\n",
    "    \"\"\"\n",
    "    book_urls = []\n",
    "    page = 1\n",
    "    base_url = \"http://books.toscrape.com/catalogue\"\n",
    "    \n",
    "    while True:\n",
    "        # Construct catalog page URL - start with page-1.html\n",
    "        page_url = f\"{base_url}/page-{page}.html\"\n",
    "        \n",
    "        try:\n",
    "            # Fetch and parse catalog page\n",
    "            response = requests.get(page_url, timeout=10)\n",
    "            if response.status_code != 200:\n",
    "                break\n",
    "                \n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            \n",
    "            # Find all book links on this catalog page\n",
    "            articles = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "            if not articles:\n",
    "                break\n",
    "                \n",
    "            for article in articles:\n",
    "                # Extract book URL from article\n",
    "                link_element = article.find(\"h3\").find(\"a\")\n",
    "                if link_element:\n",
    "                    link = link_element[\"href\"]\n",
    "                    # Convert relative to absolute URL\n",
    "                    absolute_url = f\"http://books.toscrape.com/catalogue/{link}\"\n",
    "                    book_urls.append(absolute_url)\n",
    "            \n",
    "            print(f\"Catalog page {page}: found {len(articles)} books\")\n",
    "            page += 1\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching catalog page {page}: {e}\")\n",
    "            break\n",
    "    \n",
    "    return book_urls\n",
    "\n",
    "\n",
    "def scrape_books(is_save: bool = False, max_workers: int = 10) -> List[BookData]:\n",
    "    \"\"\"\n",
    "    Scrape all books from Books to Scrape catalog using concurrent execution.\n",
    "    \n",
    "    Args:\n",
    "        is_save: If True, save results to books_data.txt\n",
    "        max_workers: Number of concurrent workers for parallel scraping\n",
    "        \n",
    "    Returns:\n",
    "        List of BookData objects for all scraped books\n",
    "    \"\"\"\n",
    "    # Phase 1: Collect all book URLs from catalog pages\n",
    "    print(\"Collecting book URLs from catalog pages...\")\n",
    "    book_urls = _collect_book_urls()\n",
    "    print(f\"Found {len(book_urls)} books to scrape\")\n",
    "    \n",
    "    if not book_urls:\n",
    "        print(\"No books found to scrape\")\n",
    "        return []\n",
    "    \n",
    "    # Phase 2: Scrape books concurrently\n",
    "    print(f\"Scraping books with {max_workers} workers...\")\n",
    "    books = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_url = {executor.submit(get_book_data, url): url for url in book_urls}\n",
    "        \n",
    "        completed = 0\n",
    "        for future in as_completed(future_to_url):\n",
    "            book_data = future.result()\n",
    "            if book_data:\n",
    "                books.append(book_data)\n",
    "            completed += 1\n",
    "            if completed % 100 == 0:\n",
    "                print(f\"Progress: {completed}/{len(book_urls)} books scraped\")\n",
    "    \n",
    "    print(f\"Successfully scraped {len(books)} books\")\n",
    "    \n",
    "    # Save to file if requested\n",
    "    if is_save:\n",
    "        output_path = Path(\"books_data.txt\")\n",
    "        \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump([asdict(book) for book in books], f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Saved to {output_path}\")\n",
    "    \n",
    "    return books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Bt7mrXcbkj5Q",
   "metadata": {
    "id": "Bt7mrXcbkj5Q"
   },
   "outputs": [],
   "source": [
    "# Test the scrape_books function with a small number of workers for testing\n",
    "print(\"Testing scrape_books function...\")\n",
    "res = scrape_books(is_save=True, max_workers=5)\n",
    "print(f\"\\nResult type: {type(res)}\")\n",
    "print(f\"Number of books scraped: {len(res)}\")\n",
    "\n",
    "if res:\n",
    "    print(\"\\nFirst book example:\")\n",
    "    print(f\"  Title: {res[0].title}\")\n",
    "    print(f\"  Price: {res[0].price}\")\n",
    "    print(f\"  Rating: {res[0].rating}\")\n",
    "else:\n",
    "    print(\"No books were scraped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z5fd728nl8a8",
   "metadata": {
    "id": "z5fd728nl8a8"
   },
   "source": [
    "## Задание 3. Настройка регулярной выгрузки (10 баллов)\n",
    "\n",
    "Настройте автоматический запуск функции сбора данных каждый день в 19:00.\n",
    "Для автоматизации используйте библиотеку `schedule`. Функция должна запускаться в указанное время и сохранять обновленные данные в текстовый файл.\n",
    "\n",
    "\n",
    "\n",
    "Бесконечный цикл должен обеспечивать постоянное ожидание времени для запуска задачи и выполнять ее по расписанию. Однако чтобы не перегружать систему, стоит подумать о том, чтобы выполнять проверку нужного времени не постоянно, а раз в какой-то промежуток. В этом вам может помочь `time.sleep(...)`.\n",
    "\n",
    "Проверьте работоспособность кода локально на любом времени чч:мм.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SajRRCj4n8BZ",
   "metadata": {
    "id": "SajRRCj4n8BZ"
   },
   "outputs": [],
   "source": [
    "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "\n",
    "def run_scheduler():\n",
    "    \"\"\"\n",
    "    Set up and run the automated daily data collection scheduler.\n",
    "    \n",
    "    The scheduler runs the book scraping function every day at 19:00 (7 PM)\n",
    "    and saves the results to books_data.txt. The function runs in an infinite\n",
    "    loop with 60-second intervals to check for scheduled tasks.\n",
    "    \n",
    "    The scheduler can be stopped with Ctrl+C (KeyboardInterrupt).\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    def scheduled_scraping():\n",
    "        \"\"\"Wrapper function for scheduled book scraping.\"\"\"\n",
    "        print(f\"\\n[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Starting scheduled book scraping...\")\n",
    "        \n",
    "        try:\n",
    "            # Run the scraping function with file saving enabled\n",
    "            books = scrape_books(is_save=True, max_workers=5)\n",
    "            print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Successfully scraped {len(books)} books\")\n",
    "            print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Data saved to books_data.txt\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Error during scheduled scraping: {e}\")\n",
    "    \n",
    "    # Schedule the scraping function to run daily at 19:00\n",
    "    schedule.every().day.at(\"19:00\").do(scheduled_scraping)\n",
    "    \n",
    "    print(\"Scheduler started! Book scraping is scheduled for 19:00 daily.\")\n",
    "    print(\"Press Ctrl+C to stop the scheduler.\")\n",
    "    \n",
    "    try:\n",
    "        # Infinite loop to check for scheduled tasks\n",
    "        while True:\n",
    "            schedule.run_pending()\n",
    "            time.sleep(60)  # Check every minute to avoid overloading the system\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nScheduler stopped by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Scheduler error: {e}\")\n",
    "\n",
    "\n",
    "def test_scheduler_with_custom_time(test_time: str = \"14:30\"):\n",
    "    \"\"\"\n",
    "    Test the scheduler functionality with a custom time for immediate verification.\n",
    "    \n",
    "    Args:\n",
    "        test_time: Time in HH:MM format to schedule the test run (default: \"14:30\")\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    def test_scraping():\n",
    "        \"\"\"Test function for scheduler verification.\"\"\"\n",
    "        print(f\"\\n[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] TEST: Starting scheduled scraping at {test_time}...\")\n",
    "        \n",
    "        try:\n",
    "            # Run a small test with just a few books\n",
    "            print(\"Running test with limited scope...\")\n",
    "            books = scrape_books(is_save=False, max_workers=2)\n",
    "            print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] TEST: Successfully scraped {len(books)} books\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] TEST: Error during test scraping: {e}\")\n",
    "    \n",
    "    # Clear any existing schedules\n",
    "    schedule.clear()\n",
    "    \n",
    "    # Schedule the test function at the specified time\n",
    "    schedule.every().day.at(test_time).do(test_scraping)\n",
    "    \n",
    "    current_time = datetime.now().strftime('%H:%M')\n",
    "    print(f\"Test scheduler started! Test scraping is scheduled for {test_time} (current time: {current_time})\")\n",
    "    print(\"Press Ctrl+C to stop the test scheduler.\")\n",
    "    \n",
    "    try:\n",
    "        # Run for a limited time to test the functionality\n",
    "        start_time = time.time()\n",
    "        while time.time() - start_time < 300:  # Run for 5 minutes max\n",
    "            schedule.run_pending()\n",
    "            time.sleep(10)  # Check every 10 seconds for testing\n",
    "            \n",
    "        print(\"Test scheduler completed (5-minute timeout)\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTest scheduler stopped by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Test scheduler error: {e}\")\n",
    "\n",
    "\n",
    "# КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e94621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the scheduler functionality\n",
    "# Note: This will run for 5 minutes to demonstrate the scheduler\n",
    "# In production, you would call run_scheduler() for continuous operation\n",
    "\n",
    "print(\"Testing scheduler with custom time (next minute)...\")\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Schedule test for 1 minute from now\n",
    "next_minute = (datetime.now() + timedelta(minutes=1)).strftime('%H:%M')\n",
    "print(f\"Scheduling test for {next_minute}\")\n",
    "\n",
    "# Run the test scheduler\n",
    "test_scheduler_with_custom_time(next_minute)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XFiPtEyaoLxq",
   "metadata": {
    "id": "XFiPtEyaoLxq"
   },
   "source": [
    "## Задание 4. Написание автотестов (15 баллов)\n",
    "\n",
    "Создайте минимум три автотеста для ключевых функций парсинга — например, `get_book_data` и `scrape_books`. Идеи проверок (можете использовать свои):\n",
    "\n",
    "* данные о книге возвращаются в виде словаря с нужными ключами;\n",
    "* список ссылок или количество собранных книг соответствует ожиданиям;\n",
    "* значения отдельных полей (например, `title`) корректны.\n",
    "\n",
    "Оформите тесты в отдельном скрипте `tests/test_scraper.py`, используйте библиотеку `pytest`. Убедитесь, что тесты проходят успешно при запуске из терминала командой `pytest`.\n",
    "\n",
    "Также выведите результат их выполнения в ячейке ниже.\n",
    "\n",
    "**Не забывайте про соблюдение PEP-8**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d4efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d0dc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lBFAw4b3z8QY",
   "metadata": {
    "id": "lBFAw4b3z8QY"
   },
   "outputs": [],
   "source": [
    "# Ячейка для демонстрации работоспособности\n",
    "# Сам код напишите в отдельном скрипте\n",
    "! pytest test/test_scraper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cRSQlHfRtOdN",
   "metadata": {
    "id": "cRSQlHfRtOdN"
   },
   "source": [
    "## Задание 5. Оформление проекта на GitHub и работа с Git (35 баллов)\n",
    "\n",
    "В этом задании нужно воспользоваться системой контроля версий Git и платформой GitHub для хранения и управления своим проектом. **Ссылку на свой репозиторий пришлите в форме для сдачи ответа.**\n",
    "\n",
    "### Пошаговая инструкция и задания\n",
    "\n",
    "**1. Установите Git на свой компьютер.**\n",
    "\n",
    "* Для Windows: [скачайте установщик](https://git-scm.com/downloads) и выполните установку.\n",
    "* Для macOS:\n",
    "\n",
    "  ```\n",
    "  brew install git\n",
    "  ```\n",
    "* Для Linux:\n",
    "\n",
    "  ```\n",
    "  sudo apt update\n",
    "  sudo apt install git\n",
    "  ```\n",
    "\n",
    "**2. Настройте имя пользователя и email.**\n",
    "\n",
    "Это нужно для подписи ваших коммитов, сделайте в терминале через `git config ...`.\n",
    "\n",
    "**3. Создайте аккаунт на GitHub**, если у вас его еще нет:\n",
    "[https://github.com](https://github.com)\n",
    "\n",
    "**4. Создайте новый репозиторий на GitHub:**\n",
    "\n",
    "* Найдите кнопку **New repository**.\n",
    "* Укажите название, краткое описание, выберите тип **Public** (чтобы мы могли проверить ДЗ).\n",
    "* Не ставьте галочку Initialize this repository with a README.\n",
    "\n",
    "**5. Создайте локальную папку с проектом.** Можно в терминале, можно через UI, это не имеет значения.\n",
    "\n",
    "**6. Инициализируйте Git в этой папке.** Здесь уже придется воспользоваться некоторой командой в терминале.\n",
    "\n",
    "**7. Привяжите локальный репозиторий к удаленному на GitHub.**\n",
    "\n",
    "**8. Создайте ветку разработки.** По умолчанию вы будете находиться в ветке `main`, создайте и переключитесь на ветку `hw-books-parser`.\n",
    "\n",
    "**9. Добавьте в проект следующие файлы и папки:**\n",
    "\n",
    "* `scraper.py` — ваш основной скрипт для сбора данных.\n",
    "* `README.md` — файл с кратким описанием проекта:\n",
    "\n",
    "  * цель;\n",
    "  * инструкции по запуску;\n",
    "  * список используемых библиотек.\n",
    "* `requirements.txt` — файл со списком зависимостей, необходимых для проекта (не присылайте все из глобального окружения, создайте изолированную виртуальную среду, добавьте в нее все нужное для проекта и получите список библиотек через `pip freeze`).\n",
    "* `artifacts/` — папка с результатами парсинга (`books_data.txt` — полностью или его часть, если весь не поместится на GitHub).\n",
    "* `notebooks/` — папка с заполненным ноутбуком `HW_03_python_ds_2025.ipynb` и запущенными ячейками с выводами на экран.\n",
    "* `tests/` — папка с тестами на `pytest`, оформите их в формате скрипта(-ов) с расширением `.py`.\n",
    "* `.gitignore` — стандартный файл, который позволит исключить временные файлы при добавлении в отслеживаемые (например, `__pycache__/`, `.DS_Store`, `*.pyc`, `venv/` и др.).\n",
    "\n",
    "\n",
    "**10. Сделайте коммит.**\n",
    "\n",
    "**11. Отправьте свою ветку на GitHub.**\n",
    "\n",
    "**12. Создайте Pull Request:**\n",
    "\n",
    "* Перейдите в репозиторий на GitHub.\n",
    "* Нажмите кнопку **Compare & pull request**.\n",
    "* Укажите, что было добавлено, и нажмите **Create pull request**.\n",
    "\n",
    "**13. Выполните слияние Pull Request:**\n",
    "\n",
    "* Убедитесь, что нет конфликтов.\n",
    "* Нажмите **Merge pull request**, затем **Confirm merge**.\n",
    "\n",
    "**14. Скачайте изменения из основной ветки локально.**\n",
    "\n",
    "\n",
    "\n",
    "### Требования к итоговому репозиторию\n",
    "\n",
    "* Файл `scraper.py` с рабочим кодом парсера.\n",
    "* `README.md` с описанием проекта и инструкцией по запуску.\n",
    "* Папка `artifacts/` с результатом сбора данных (`.txt` файл).\n",
    "* Папка `tests/` с тестами на `pytest`.\n",
    "* Папка `notebooks/` с заполненным ноутбуком `HW_03_python_ds_2025.ipynb`.\n",
    "* Pull Request с комментарием из ветки `hw-books-parser` в ветку `main`.\n",
    "* Примерная структура:\n",
    "\n",
    "  ```\n",
    "  books_scraper/\n",
    "  ├── artifacts/\n",
    "  │   └── books_data.txt\n",
    "  ├── notebooks/\n",
    "  │   └── HW_03_python_ds_2025.ipynb\n",
    "  ├── scraper.py\n",
    "  ├── README.md\n",
    "  ├── tests/\n",
    "  │   └── test_scraper.py\n",
    "  ├── .gitignore\n",
    "  └── requirements.txt\n",
    "  ```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
